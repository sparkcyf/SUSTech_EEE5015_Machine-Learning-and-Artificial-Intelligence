{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autograd\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data\n",
    "torch.manual_seed(4321)\n",
    "X = torch.rand(size=(8, 2))\n",
    "y = torch.randint(low=0, high=3, size=(8,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1255, 0.5377],\n",
      "        [0.6564, 0.0365],\n",
      "        [0.5837, 0.7018],\n",
      "        [0.3068, 0.9500],\n",
      "        [0.4321, 0.2946],\n",
      "        [0.6015, 0.1762],\n",
      "        [0.9945, 0.3177],\n",
      "        [0.9886, 0.3911]])\n",
      "tensor([0, 2, 2, 0, 2, 2, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creatre a Vanilla model with 2 inputs, 3 outputs, and 1 hidden layer with 2 nodes and bias on the hidden layer and output layer\n",
    "class Vanilla(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vanilla, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2, bias=True)\n",
    "        self.fc2 = nn.Linear(2, 3, bias=True)\n",
    "        self.logistic_activate_func = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    # specify the initial weight\n",
    "    def init_weight(self):\n",
    "        self.fc1.weight.data = torch.tensor([[0.48, -0.51], [-0.43, -0.48]], dtype=torch.float)\n",
    "        self.fc1.bias.data = torch.tensor([0.23, 0.05], dtype=torch.float)\n",
    "        self.fc2.weight.data = torch.tensor([[-0.99, -0.66], [0.36, 0.34], [-0.75, 0.66]], dtype=torch.float)\n",
    "        self.fc2.bias.data = torch.tensor([0.32, -0.44, 0.70], dtype=torch.float)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.logistic_activate_func(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1867, 0.2663, 0.5470],\n",
      "        [0.1747, 0.2958, 0.5295],\n",
      "        [0.1959, 0.2738, 0.5303],\n",
      "        [0.2022, 0.2590, 0.5388],\n",
      "        [0.1812, 0.2820, 0.5368],\n",
      "        [0.1787, 0.2902, 0.5311],\n",
      "        [0.1863, 0.2966, 0.5171],\n",
      "        [0.1886, 0.2943, 0.5171]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor(1.0681, grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0057,  0.0067],\n",
      "        [-0.0017,  0.0058]])\n",
      "tensor([0.0167, 0.0001])\n",
      "tensor([[-0.0059, -0.0053],\n",
      "        [ 0.0323,  0.0252],\n",
      "        [-0.0264, -0.0199]])\n",
      "tensor([-0.0157,  0.0579, -0.0422])\n"
     ]
    }
   ],
   "source": [
    "# use and prediction the model\n",
    "model = Vanilla()\n",
    "model.init_weight()\n",
    "print(model(X))\n",
    "\n",
    "# loss\n",
    "loss = nn.CrossEntropyLoss()\n",
    "print(loss(model(X), y))\n",
    "\n",
    "# calculate the gradient\n",
    "model.zero_grad()\n",
    "loss(model(X), y).backward()\n",
    "\n",
    "# print the gradient\n",
    "print(model.fc1.weight.grad)\n",
    "print(model.fc1.bias.grad)\n",
    "print(model.fc2.weight.grad)\n",
    "print(model.fc2.bias.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:\n",
      "tensor([0.1867, 0.1747, 0.1959, 0.2022, 0.1812, 0.1787, 0.1863, 0.1886],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([0.2273, 0.2463, 0.2321, 0.2225, 0.2374, 0.2426, 0.2465, 0.2450],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([0.4277, 0.4045, 0.4092, 0.4214, 0.4142, 0.4070, 0.3917, 0.3921],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([0.8416, 0.8255, 0.8371, 0.8460, 0.8327, 0.8283, 0.8245, 0.8257],\n",
      "       grad_fn=<AddBackward0>)\n",
      "loss:\n",
      "tensor(3.9537, grad_fn=<SubBackward0>)\n",
      "grad:\n",
      "(tensor([0.0085, 0.0081, 0.0075, 0.0080, 0.0081, 0.0080, 0.0074, 0.0072],\n",
      "       grad_fn=<AddBackward0>), tensor([0.0334, 0.0315, 0.0337, 0.0342, 0.0327, 0.0321, 0.0320, 0.0322],\n",
      "       grad_fn=<AddBackward0>))\n",
      "(tensor([0.0176, 0.0169, 0.0173, 0.0170, 0.0175, 0.0172, 0.0164, 0.0165],\n",
      "       grad_fn=<AddBackward0>), tensor([-0.0193, -0.0191, -0.0196, -0.0191, -0.0195, -0.0193, -0.0191, -0.0193],\n",
      "       grad_fn=<AddBackward0>))\n",
      "(tensor([-0.0423, -0.0401, -0.0406, -0.0411, -0.0414, -0.0406, -0.0386, -0.0386],\n",
      "       grad_fn=<AddBackward0>), tensor([0.0027, 0.0014, 0.0036, 0.0035, 0.0025, 0.0020, 0.0022, 0.0026],\n",
      "       grad_fn=<AddBackward0>))\n"
     ]
    }
   ],
   "source": [
    "x1 = X[:, 0].requires_grad_(True)\n",
    "x2 = X[:, 1].requires_grad_(True)\n",
    "\n",
    "# Forward\n",
    "b10 = 0.23\n",
    "b20 = 0.05\n",
    "\n",
    "a11 = 1 / (1 + torch.exp(-(0.48 * x1 - 0.51 * x2 + b10)))\n",
    "a21 = 1 / (1 + torch.exp(-(-0.43 * x1 - 0.48 * x2 + b20)))\n",
    "\n",
    "b11 = 0.32\n",
    "b21 = -0.44\n",
    "b31 = 0.70\n",
    "\n",
    "y1 = a11 * -0.99 + a21 * -0.66 + b11\n",
    "y2 = a11 * 0.36 + a21 * 0.34 + b21\n",
    "y3 = a11 * -0.75 + a21 * 0.66 + b31\n",
    "\n",
    "# softmax\n",
    "y1 = torch.exp(y1) / (torch.exp(y1) + torch.exp(y2) + torch.exp(y3))\n",
    "y2 = torch.exp(y2) / (torch.exp(y1) + torch.exp(y2) + torch.exp(y3))\n",
    "y3 = torch.exp(y3) / (torch.exp(y1) + torch.exp(y2) + torch.exp(y3))\n",
    "\n",
    "# categorical cross entropy loss\n",
    "loss = (\n",
    "    - torch.log(y1[y == 0]).mean()\n",
    "    - torch.log(y2[y == 1]).mean()\n",
    "    - torch.log(y3[y == 2]).mean()\n",
    ")\n",
    "\n",
    "dy1_dx = autograd.grad(\n",
    "    outputs=y1, inputs=[x1, x2], grad_outputs=torch.ones_like(y1), create_graph=True\n",
    ")\n",
    "dy2_dx = autograd.grad(\n",
    "    outputs=y2, inputs=[x1, x2], grad_outputs=torch.ones_like(y2), create_graph=True\n",
    ")\n",
    "dy3_dx = autograd.grad(\n",
    "    outputs=y3, inputs=[x1, x2], grad_outputs=torch.ones_like(y3), create_graph=True\n",
    ")\n",
    "\n",
    "print(\"prediction:\")\n",
    "print(y1)\n",
    "print(y2)\n",
    "print(y3)\n",
    "print(y1+y2+y3)\n",
    "print(\"loss:\")\n",
    "print(loss)\n",
    "print(\"grad:\")\n",
    "print(dy1_dx)\n",
    "print(dy2_dx)\n",
    "print(dy3_dx)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eee5015-py39-torchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a305ffb038d935178ec6e4c02363eb290c7a5404d7bc77dc897f9947beff9331"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
